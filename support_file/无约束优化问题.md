无约束优化问题的定义如下： 

$ min_{x}f(x) $

# 1.梯度下降法
<font style="color:rgb(77, 77, 77);">梯度下降法是一种基于迭代的一阶优化方法。</font>梯度下降法是**沿着负梯度方向**进行优化变量的更新，具体方式为：

$  x_{k+1}=x_{k}−\tau \nabla f(x_k) $

其中，$  k $ 代表迭代轮数。由于负梯度方向只能保证在$  x_k $的一个局部邻域内目标函数值的下降，所以通常需要搭配一个步长系数$ \tau $。

如何选择 $ \tau
 $ 对于优化过程非常重要。步长系数$ \tau $的选择过程也被称为线搜索过程，即沿着优化方向，搜索一个合适的步长的过程，通常分为精确线搜索和非精确线搜索。其中，精确线搜索需要寻找到沿着优化方向使得目标函数值最优的步长，而非精确线搜索则是仅仅需要找到一个符合某种条件的步长即可。一般来说，非精确线搜索在工程领域是更加常用的。这里介绍一个在非精确线搜索中常见的条件，也就是Armijo Condition，其具体表示形式如下：

 $ f(x^k)−f(x^k+\alpha d) ≥ −c \alpha d^T \nabla f(x^k) $

其中， $ \alpha $代表步长， $ d $代表优化方向， $ c $代表一个可调参数，通常取$  c\in(0,1) $。

因此，梯度下降法在工程上的实现通常就是采用负梯度方向作为优化方向，结合Armijo条件计算步长。其流程为：

> 1. 使用梯度作为梯度方向：$ d=-\nabla f(x_k) $
> 2. 使用 Armijo 条件寻找搜索步长：While  $ f(x^k)−f(x^k+\alpha d) ≥ −c \alpha d^T \nabla f(x^k) $ do $ \tau = \tau /2 $
> 3. 更新迭代：$ x_{k+1} = x_{k} + \tau d  $
>

![](https://cdn.nlark.com/yuque/0/2024/jpeg/22737080/1734876943260-2232bce0-3bd1-4e42-86d8-a42e8922144c.jpeg)



# 2. 牛顿法
<font style="color:rgb(77, 77, 77);">牛顿法是基于迭代的二阶优化方法，</font>牛顿法是**沿着牛顿方向**进行优化变量的更新，**牛顿方向是综合考虑局部邻域内的一、二阶微分信息，得出的优化方向**，具体形式如下： 

$ x_{k+1}=x_k−[ \nabla^2 f(x_k)]^{−1} \nabla f(x_k) $

其中， $ \nabla^2 f(x_k) $ 代表Hessian矩阵。

## 2.1 牛顿法迭代过程
![目标函数（凸函数，起码是局部凸函数）](https://cdn.nlark.com/yuque/0/2025/png/22737080/1740017627898-78e557ce-8c54-4b8c-a82f-4abfdf9323ac.png)![目标函数的一阶导数](https://cdn.nlark.com/yuque/0/2025/png/22737080/1740017636458-b47844f6-4477-4e2c-a237-679506b10151.png)

目标函数为凸函数（起码是局部凸函数），在其一阶导数上进行优化，下图动态展示了这个过程，因此牛顿法求解的过程可以这样描述：

> a. 在X轴上随机一点,经过做X轴的垂线,得到垂线与函数图像的交点.
>
> b.  通过做函数的切线,得到切线与X轴的交点.
>
> c. 迭代 a，b 两步，当前后两次求的x相同或者两个值的差小于一个阈值的时候，我们就认为找到了
>

![牛顿法优化过程](https://cdn.nlark.com/yuque/0/2025/gif/22737080/1740017582654-f8311928-4ce2-4474-89b8-ce003e6f35c6.gif)

## 2.2 牛顿法公式推导
![](https://cdn.nlark.com/yuque/0/2025/png/22737080/1740018164685-4cc02c5d-bae8-49a5-8487-f917e2f55060.png)

图中蓝色的线是 $ f(x)  $函数的的导数 $ \nabla f(x_k) $，则曲线在$  x_1  $处的导数为 $ \nabla^2 f(x_1) $，我们要求，根据三角函数有：

$ \nabla^2 f(x_1) = \frac{\nabla f(x_1)}{x_1-x_2} $

得出：

$ x_2 = x_1 + [\nabla^2f(x_1)]^{-1}\nabla f(x_1) $

 利用开始进行下一轮的迭代。迭代公式可以简化如下：

$ x_{k+1} = x_{k} + [\nabla^2f(x_k)]^{-1}\nabla f(x_k) $

## 2.3 牛顿法缺陷
牛顿法在一些问题上相比于梯度下降法有着更快的收敛速度，但是牛顿法也有一些缺点。比较显然的是牛顿法要求**Hessian矩阵非奇异且正定**，另外**Hessian矩阵的获取和求逆计算可能会非常耗时**。

+ Hessian矩阵非奇异。如果 Hessian 是奇异矩阵，则无法求逆，也就无法求取牛顿方向。
+ Hessian矩阵需要正定。如果Hessian矩阵不是正定时，无法保证牛顿方向是下降方向。

> 如果Hessian矩阵正定，那么它的特征值全是正的，这意味着目标函数在该点是局部凸的，从而牛顿方向一定是下降方向。反之，如果Hessian矩阵不是正定的，比如它是半正定或者负定，则可能出现以下几种情况：
>
> + **半正定Hessian矩阵**：部分方向上的二阶导数为零，牛顿方向可能变得“平坦”，导致算法停滞，或者在某些方向上没有下降的保证。
> + **负定Hessian矩阵**：目标函数在该点是局部凹的，因此牛顿法可能会在该点沿着一个上升的方向搜索，从而无法保证下降。
> + **不定Hessian矩阵**：既有正的特征值也有负的特征值，这意味着目标函数在某些方向上是凸的，在某些方向上是凹的，牛顿法可能无法稳定收敛，甚至可能会出现震荡。
>

实际应用中，Hessian矩阵正定通常是难以保证的。因此，工程实践中会有一些解决办法。

+ 当Hessian矩阵是半正定时，只需要加入一个很小的单位阵就可以将Hessian矩阵改造为正定矩阵，具体改造方法为： $ M=\nabla^2f(x) + \epsilon I,~\epsilon=\min(1,||\nabla f(x)||_{\infty})/10 $。其好处是，当$  \nabla f(x) $ 接近于0时， $ M $ 也会逐渐接近于真实的Hessian矩阵。另外，由于改造过后的Hessian矩阵是正定的，可以采用Cholesky分解的方式，来求解牛顿方向： $ Md=-\nabla f(x) $ ，$ M=LL^T $ ，从而避免耗时的求逆操作。
+ 当Hessian矩阵是不定时，可以采用Bunch-Kaufman分解，即 $ M=LBL^T=\nabla^2f(x) $ 。其中， B 矩阵是一个块对角矩阵，都是由$ 1 \times 1 $和$ 2 \times 2 $的小块矩阵构成的，可以很容易地去修改这些小块矩阵的特征值，把他们改成正数。

# 3 拟牛顿法
牛顿法存在一些缺点：不保证一定是下降方向，二阶导数信息获取成本高，需要求解一个线性方程组。因此，研究者提出了拟牛顿方法。**拟牛顿法的思想是尽可能采用一阶信息去还原出二阶信息，从而实现超出一阶方法的收敛速度，同时避免求解目标函数的二阶导数。**

## 3.1 拟牛顿法-BFGS算法
为了实现上述目标，可以考虑一阶导数的泰勒展开：

$  \nabla f(x_{k+1}) \approx \nabla f(x_k) + \nabla^2 f(x_k) (x_{k+1}-x_{k}) $

为了避免显式计算$ \nabla^2 f(x) $，拟牛顿法通过一阶信息（梯度）近似构造二阶信息。由上式变化得到：

$ \nabla f(x_{k+1}) - \nabla f(x_k) \approx \nabla^2 f(x_k)(x_{k+1} - x_k) $

使用$ \Delta x_k = x_{k+1} - x_k $表示优化过程中从第 k 次迭代点 $ x_k $ 到下一次迭代点 $ x_{k+1} $ 的步长；$ \Delta g_k = \nabla f(x_{k+1}) - \nabla f(x_k) $表示梯度的变化量。因此可以改写为：

 $ \Delta g_k \approx \nabla^2 f(x_k) \Delta x_k $

反过来，定义近似的 Hessian 矩阵的逆：

$ B_{k+1} \approx [\nabla^2 f(x_k)]^{-1} $

最终得到：

$ \Delta x_k \approx B_{k+1} \Delta g_k $

这就是 Secant Equation，它是拟牛顿法构造更新矩阵 $ B_{k+1} $ 的核心公式。满足Secant Equation的 $ B $ 矩阵有无穷多个，如何选择其中最优的一个呢？可以通过下面的这个优化问题得到： 

$ \begin{aligned} &\min_B ||H^{1/2}(B-B^k)H^{1/2}||^2\\ &s.t.~B=B^T\\ &~~~~~~~\Delta x=B\Delta g\\ \end{aligned}\\ $

+ 目标函数 : 加权 Frobenius 范数，用于最小化新的$ B $与当前$ B_k $之间的差距，保持迭代更新的稳定性。并使用$ H^{1/2} $进行加权。
+ 约束条件: $ B = B^T $要求 B 是对称矩阵，与 Hessian 的性质一致。$ \Delta x = B \Delta g $确保 $ B $ 符合 Secant Equation 的约束条件。

对上述优化问题进行求解，就可以得到大名鼎鼎的拟牛顿法-BFGS算法的更新过程。 

$ B_{k+1}=\left(I-\frac{\Delta x\Delta g^T}{\Delta g^T\Delta x}\right) B_k \left(I-\frac{\Delta g\Delta x^T}{\Delta g^T\Delta x}\right)+\frac{\Delta x\Delta x^T}{\Delta g^T\Delta x}\\ $

 其中， $ B_0=I,~\Delta x=x_{k+1}-x_k,~\Delta g_k=\nabla f(x_{k+1})-\nabla f(x_k) $。这是一个迭代更新的过程，每一轮迭代优化步都需要对 B 矩阵进行更新，从而用历史的一阶梯度信息，对二阶信息进行迭代估计。

### 3.1.1 Weak Wolfe 条件
BFGS更新过程无法保证 $ B_{k+1}  $正定，因此不能保证目标函数下降，除非满足不等式： $ \Delta g^T\Delta x>0 $ 。为了使该不等式成立，可以在线搜索过程中，在Armjio条件的基础上加入新的一个条件(曲率条件)，从而得到了 Weak Wolfe conditions，如下：

$ f(x^k)-f(x^k+\alpha d) \geq -c_1\cdot\alpha d^T \nabla f(x^k)\\ d^T\nabla f(x^k+\alpha d) \geq c_2\cdot d^T\nabla f(x^k)\\ $

+ Armijo 条件确保步长 $ \alpha $ 足够小，使得目标函数 $ f(x) $ 沿搜索方向 $ d $ 实现一定程度的下降。这保证了步长选择可以显著降低目标函数值，确保搜索方向的有效性。
+ 曲率条件确保新的梯度 $ \nabla f(x^k + \alpha d) $ 沿搜索方向 $ d $ 的投影不“反弹”过多，即保持下降趋势。这确保新的梯度与搜索方向保持适当的关系，防止过短步长或误导性更新。

如果仅满足 Armijo 条件，可能会选取一个过小的步长，从而导致搜索效率低下。曲率条件限制了步长过小的情况，确保每次更新都能有效调整优化方向。

Weak Wolfe 条件的目标是：

+ 确保目标函数的下降（通过 Armijo 条件）。
+ 确保搜索方向的合理性（通过曲率条件），即新的梯度不会破坏当前的下降方向。
+ 保证正定性：如果 $ d^T \nabla f(x^k + \alpha d) \geq 0 $，就有$ \Delta g^T \Delta x > 0 $，从而 $ B_{k+1} $ 保持正定性。

Weak Wolfe Conditions 的实现一般结合线搜索算法进行：

> 1. 初始化步长 $ \alpha $。
> 2. 检查 Armijo 条件：
>     - 若不满足，减小 $ \alpha $（如二分法）。
> 3. 检查曲率条件：
>     - 若不满足，$ \alpha $。
> 4. 当两个条件都满足时，返回合适的步长 $ \alpha $。
>

### 3.1.2 Cautious-BFGS 算法
 Weak Wolfe conditions条件虽然能保证函数下降，但$ B_{k+1}  $的正定性取决于$ \Delta g^T \Delta x > 0 $，若该条件被破坏，$ B_{k+1}  $可能失去正定性，导致搜索方向无效。如果$ \Delta g^T \Delta x \approx 0 $，会导致分母过小，出现数值不稳定。因此不能保证最终收敛，要想保证最终收敛，有研究人员提出了BFGS update的升级版本Cautious-BFGS 算法。

Cautious-BFGS 算法是 BFGS 算法的一种改进方法，用于处理标准 BFGS 更新可能导致的数值不稳定性问题。特别是在梯度变化 $ \Delta g^k $很小或者更新方向 $ \Delta x^k $ 和梯度变化 $ \Delta g^k $不够充分时，标准 BFGS 更新公式可能会生成非正定或不合理的近似矩阵 $ B^{k+1} $。为了避免这些问题，Cautious Update 引入了更稳健的更新策略。

设定一个阈值$ \epsilon > 0 $（通常是一个小正数，例如 $ 10^{-8} $）检查正定性条件：

$ \Delta g^T \Delta x > \epsilon $

如果 $ \Delta g^T \Delta x $小于阈值，则认为当前步的更新信息不足，不执行标准的 BFGS 更新，而是对$ B^{k+1} $进行保守修正。

在 $ \Delta g^T \Delta x $不满足正定性条件时，Cautious Update 使用以下策略：

+ 缩放修正：调整更新步长，减少更新对 $ B_{k+1}  $的影响。$ B^{k+1} = \rho B^k + (1-\rho)I $。其中 $ \rho \in [0, 1] $ 是缩放因子，用于平衡原始 $ B_k $和单位矩阵 $ I $。
+ 投影修正：约束更新矩阵的谱范围，强制矩阵保持正定性。
+ 保守更新：仅更新某些成分，而不执行完全更新，或直接保持 $ B^{k+1} = B^k $。

因此，采用 保守更新的 BFGS update的升级版本cautious update具体如下：

$ B^{k+1}=\left\{\begin{aligned} &\left(I-\frac{\Delta x\Delta g^T}{\Delta g^T\Delta x}\right) B^k\left(I-\frac{\Delta g\Delta x^T}{\Delta g^T\Delta x}\right)+\frac{\Delta x\Delta x^T}{\Delta g^T\Delta x},~if~\Delta g^T\Delta x>\epsilon||g_k||\Delta x^T\Delta x,~\epsilon=10^{-8}\\ &B^k,~otherwise \end{aligned}\right.\\ $

## 3.2 L-BFGS算法
cautious update依然面临一个问题，就是当 $ x_k $ 距离最优解非常远时，对 $ B_k  $的估计意义不大，对收敛速度也没有什么帮助。而且每次迭代计算需要前次迭代得到的，存储空间至少为N(N+1)/2（N为特征维数），对于高维的应用场景，需要的存储空间将是非常巨大的。为了解决这个问题，就有了L-BFGS算法。L-BFGS即Limited-memory BFGS。 L-BFGS的基本思想就是通过存储前m次迭代的少量数据来替代前一次的矩阵，从而大大减少数据的存储空间。这个时候，我们可以只利用最近的 $ m  $轮梯度信息对 $ B^k  $进行估计，这就进一步延伸出了Limited-memory BFGS（L-BFGS）算法。

L-BFGS算法只存储最近的 $ m $ 对 $ (\Delta x_k, \Delta g_k) $，不再累积所有历史信息，降低了存储复杂度。当 $ m $ 比较小时，算法在内存和计算效率上表现更优。L-BFGS算法不直接计算和存储 $ B_k $ 或 $ H_k $，而是通过递归方式用这些向量近似计算更新方向。

L-BFGS算法流程

> 1. 初始化： $ (x_0, g_0 \leftarrow \nabla f(x_0), B_0 \leftarrow I, k \leftarrow 0 ) $
> 2. <font style="color:rgba(0, 0, 0, 0.75);">进入循环：</font>
>     1. <font style="color:rgba(0, 0, 0, 0.75);">计算搜索方向 </font>$ d \leftarrow -B^k g^k  $
>     2. 使用Weak Wolfe 条件线搜索找到步长$ \tau $。
>     3. 更新变量 $ x_{k+1} \leftarrow x_k + \tau \cdot d  $
>     4. 更新梯度$ g_{k+1} \leftarrow \nabla f(x_{k+1}) $
>     5. 使用 Cautious-Limited-Memory-BFGS 更新近似 Hessian 信息$ B_{k+1} $
>     6. 检查是否满足收敛条件或停止准则
> 3. 返回最优解$ x^*  $和对应的目标函数值 $ f(x^*)  $
>

```cpp
#include <iostream>
#include <Eigen/Dense>
#include <vector>
#include <cmath>
#include <limits>

using namespace std;
using namespace Eigen;

class LBFGS {
public:
    LBFGS(function<double(const VectorXd&)> func, function<VectorXd(const VectorXd&)> grad_func, int m = 10, double tol = 1e-6, int max_iter = 100)
            : func(func), grad_func(grad_func), m(m), tol(tol), max_iter(max_iter) {}

    // 线搜索方法：使用弱 Wolfe 条件
    double line_search(const VectorXd& x, const VectorXd& g, const VectorXd& d) {
        double alpha = 1.0;
        double c1 = 1e-4;
        double c2 = 0.9;
        double f_x = func(x);
        double grad_dot_d = g.dot(d);

        // 线搜索
        while (func(x + alpha * d) > f_x + c1 * alpha * grad_dot_d) {
            alpha *= 0.5;
        }
        return alpha;
    }

    // 使用Cautious-Limited-Memory-BFGS更新Hessian近似
    MatrixXd update_hessian_approximation(const VectorXd& s, const VectorXd& y, const MatrixXd& B) {
        double rho = 1.0 / y.dot(s);
        MatrixXd I = MatrixXd::Identity(s.size(), s.size());
        
        MatrixXd Bs = B * s * s.transpose() * B.transpose();
        MatrixXd outer_sy = s * y.transpose();
        
        // Hessian近似更新
        MatrixXd B_new = B + (1 + y.dot(s) / (y.dot(s))) * outer_sy * rho;
        B_new -= Bs * rho;
        
        return B_new;
    }

    // LBFGS优化主过程
    VectorXd optimize(const VectorXd& x0) {
        VectorXd x_k = x0;
        VectorXd g_k = grad_func(x_k);
        MatrixXd B_k = MatrixXd::Identity(x_k.size(), x_k.size());
        vector<VectorXd> s_list;
        vector<VectorXd> y_list;

        int k = 0;

        while (k < max_iter) {
            VectorXd d_k = -B_k * g_k;  // 计算搜索方向

            // 线搜索找到步长
            double tau = line_search(x_k, g_k, d_k);

            // 更新x_{k+1}
            VectorXd x_k1 = x_k + tau * d_k;
            VectorXd g_k1 = grad_func(x_k1);

            // 计算s = x_{k+1} - x_k 和 y = g_{k+1} - g_k
            VectorXd s_k = x_k1 - x_k;
            VectorXd y_k = g_k1 - g_k;

            // 更新Hessian近似
            B_k = update_hessian_approximation(s_k, y_k, B_k);

            // 更新s_list和y_list
            if (s_list.size() >= m) {
                s_list.erase(s_list.begin());
                y_list.erase(y_list.begin());
            }

            s_list.push_back(s_k);
            y_list.push_back(y_k);

            // 检查收敛条件
            if (g_k1.norm() < tol) {
                break;
            }

            // 更新变量
            x_k = x_k1;
            g_k = g_k1;
            k++;
        }

        return x_k;
    }

private:
    function<double(const VectorXd&)> func;
    function<VectorXd(const VectorXd&)> grad_func;
    int m;  // BFGS记忆长度
    double tol;  // 收敛容忍度
    int max_iter;  // 最大迭代次数
};

// 示例目标函数和梯度
double func(const VectorXd& x) {
    return 0.5 * x.dot(x);  // 简单的二次函数 f(x) = 0.5 * ||x||^2
}

VectorXd grad_func(const VectorXd& x) {
    return x;  // 梯度是 f'(x) = x
}

int main() {
    // 设置初始点
    VectorXd x0(2);
    x0 << 1.0, 1.0;  // 初始点

    LBFGS lbfgs(func, grad_func);
    VectorXd x_opt = lbfgs.optimize(x0);

    cout << "最优解: " << x_opt.transpose() << endl;
    cout << "最优目标函数值: " << func(x_opt) << endl;

    return 0;
}
```

L-BFGS-Lite 库使用

[GitHub - ZJU-FAST-Lab/LBFGS-Lite: LBFGS-Lite: A header-only L-BFGS unconstrained optimizer.](https://github.com/ZJU-FAST-Lab/LBFGS-Lite)

# 4 优化算法的收敛速度分析
评估一个优化算法的收敛速度主要评估的是**该算法需要经过几轮迭代才能最终收敛至最优解附近**。$ x^*  $代表最优解，定义误差： $ e^k=x^k -x^* $ 其中，线性、二次、超线性收敛速度的定义如下：

+ 线性收敛速度：$  ||e^{k+1}||=C||e^k|| ， C < 1  $，这也意味着$  k  $轮以后，误差会小于$  e^k <C^ke^0 $ 。如果对这个误差取对数的话，$  \log e^k < \log C \cdot k + \log e^0  $，这表明误差和迭代轮数$  k  $成线性关系。
+ 二次收敛速度：$ ||e^{k+1}||=C||e^k||^2 $
+ 超线性收敛速度： $ ||e^{k+1}||=C||e^k||^p，p>1 $

如果要分析一个算法收敛速度的话，可以画出横坐标为迭代轮数，纵坐标为$ log(err) $的图像。如果是一条直线，说明是线性收敛速度。如果是一条向下的二次曲线，说明是二次收敛速度。各个算法在计算复杂度和收敛速度上的关系如图所示，纵轴表示收敛速度的快慢，横轴表示单轮迭代计算量的大小。

![](https://cdn.nlark.com/yuque/0/2025/png/22737080/1735981069336-3151795d-aa89-4544-9e5c-9fe23042a6cf.png)









### 
